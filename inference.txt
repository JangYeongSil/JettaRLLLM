# IMPORTANT NOTICE : IF THIS DOESN'T WORK USE THE OTHER INFERENCE FILE CODE .



def generate_text(model, start_text, token_to_idx, idx_to_token, max_length=50):
    model.eval()
    # Preprocess the input text
    input_indices = [token_to_idx.get(token, token_to_idx['<unk>']) for token in start_text.split()]
    input_tensor = torch.tensor([input_indices], dtype=torch.long).to(device)

    generated_tokens = []

    with torch.no_grad():
        for _ in range(max_length):
            # Pass input through the model to get logits
            logits, _ = model(input_tensor)

            # Get the logits for the last time step
            last_logits = logits[:, -1, :]

            # Apply softmax to get probabilities
            probabilities = torch.softmax(last_logits, dim=-1).squeeze(0)

            # Prevent generating unknown tokens ('<unk>')
            probabilities[token_to_idx['<unk>']] = 0

            # Normalize probabilities after adjustment
            probabilities /= torch.sum(probabilities)

            # Sample the next token
            next_token_id = torch.multinomial(probabilities, 1).item()
            next_token = idx_to_token.get(next_token_id, '<unk>')

            if next_token == '<eos>':  # Stop if end of sequence is generated
                break

            generated_tokens.append(next_token)

            # Append the new token to the input tensor for the next iteration
            input_tensor = torch.cat([input_tensor, torch.tensor([[next_token_id]], device=device)], dim=1)

    return ' '.join(generated_tokens)

# Usage example (same as before):
start_text = "I have"
max_length = 100
generated_text = generate_text(model, start_text, vocab, reverse_vocab, max_length)
print("Generated text:", generated_text)
